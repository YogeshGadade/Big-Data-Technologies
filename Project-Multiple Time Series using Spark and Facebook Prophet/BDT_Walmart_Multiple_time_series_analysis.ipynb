{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDT Walmart Multiple time series analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Time Series Model Using Apache Spark and Facebook Prophet\n",
        "\n",
        "\n",
        "### Yogesh Awdhut Gadade\n",
        "\n",
        "#### Goal: Multiple time series analysis forecasting.\n",
        "\n",
        "### Platform: Google Collab. One can try Databricks and AWS too."
      ],
      "metadata": {
        "id": "NJcefQK60xI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOD7GL_Ri-fi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from fbprophet import Prophet\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "plt.rcParams['axes.grid'] = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "kn91YHkfjXze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "spark = SparkSession.builder.master('local').getOrCreate()"
      ],
      "metadata": {
        "id": "2wNlmdC9jQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sample_data/weekly_sales_data.csv\")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Aw2XNDGVjbJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "Ew5DK-h7dZxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Q99UtQADjn1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_df = df.set_index('date')\n",
        "item_df.query('store_id == 25')[['sales']].plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZN_P3Eshj2We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_df.query('store_id == 41')[['sales']].plot(figsize=(15, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KVzqd6xfj4KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = spark.createDataFrame(df)\n",
        "sdf.printSchema() #data type of each col\n",
        "sdf.show(5) #It gives you head of pandas DataFrame\n",
        "sdf.count() #500 records"
      ],
      "metadata": {
        "id": "joeSVbG7j4ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.select(['store_id']).groupby('store_id').agg({'store_id': 'count'}).show()"
      ],
      "metadata": {
        "id": "8253Gsswj4bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.createOrReplaceTempView(\"sales\")\n",
        "spark.sql(\"select store_id, count(*) from sales group by store_id order by store_id\").show()"
      ],
      "metadata": {
        "id": "Zu8mCfGBkAnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"SELECT store_id, date as ds, sum(sales) as y FROM sales GROUP BY store_id, ds ORDER BY store_id, ds\"\n",
        "spark.sql(sql).show()"
      ],
      "metadata": {
        "id": "V50BEl4CkAqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_part = (spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['store_id'])).cache()\n",
        "sdf.explain()"
      ],
      "metadata": {
        "id": "Q4-7y1TTkAsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "result_schema = StructType([\n",
        "                  StructField('ds', TimestampType()),\n",
        "                  StructField('store_id', IntegerType()),\n",
        "                  StructField('y', DoubleType()),\n",
        "                  StructField('yhat', DoubleType()),\n",
        "                  StructField('yhat_upper', DoubleType()),\n",
        "                  StructField('yhat_lower', DoubleType())\n",
        "])"
      ],
      "metadata": {
        "id": "0auhDxPskMFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
        "def forecast_sales(store_pd):\n",
        "  model = Prophet(interval_width=0.95, seasonality_mode= 'multiplicative', weekly_seasonality=True, yearly_seasonality=True)\n",
        "  model.fit(store_pd)\n",
        "  future_pd = model.make_future_dataframe(periods=5, freq='w')\n",
        "  forecast_pd = model.predict(future_pd)\n",
        "  f_pd = forecast_pd[['ds', 'yhat', 'yhat_upper', 'yhat_lower']].set_index('ds')\n",
        "  st_pd = store_pd[['ds', 'store_id', 'y']].set_index('ds')\n",
        "  result_pd = f_pd.join(st_pd, how='left')\n",
        "  result_pd.reset_index(level=0, inplace=True)\n",
        "  result_pd['store_id'] = store_pd['store_id'].iloc[0]\n",
        "  return result_pd[['ds', 'store_id', 'y', 'yhat', 'yhat_upper', 'yhat_lower']]"
      ],
      "metadata": {
        "id": "t8YccidMkMIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date\n",
        "results = (store_part.groupby('store_id').apply(forecast_sales).withColumn('training_date', current_date()))\n",
        "results.cache()\n",
        "results.show()"
      ],
      "metadata": {
        "id": "-GtN75eTkMK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.coalesce(1)\n",
        "print(results.count())\n",
        "results.createOrReplaceTempView('forecasted')\n",
        "spark.sql(\"SELECT store_id, count(*) FROM  forecasted GROUP BY store_id\").show()"
      ],
      "metadata": {
        "id": "BUcsbjZQkatZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = results.toPandas()"
      ],
      "metadata": {
        "id": "nPLv7hMbkawL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.set_index('ds')"
      ],
      "metadata": {
        "id": "PyrmK0Qek1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for store_id in list(final_df.store_id.unique()):\n",
        "  final_df.query('store_id == {}'.format(store_id))[['y', 'yhat']].plot()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ooW76oRDzDlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source:\n",
        "\n",
        "\n",
        "\n",
        "*   https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3\n",
        "* https://www.analyticsvidhya.com/blog/2022/01/apache-spark-and-facebook-prophet/\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "qnTW7lmsFGKd"
      }
    }
  ]
}